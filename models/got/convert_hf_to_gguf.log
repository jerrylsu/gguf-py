INFO:hf-to-gguf:Loading model: got-ocr2_0
INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'
INFO:hf-to-gguf:choosing --outtype bf16 from first tensor type (torch.bfloat16)
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'
INFO:hf-to-gguf:output.weight,                    torch.bfloat16 --> BF16, shape = {1024, 151860}
INFO:hf-to-gguf:token_embd.weight,                torch.bfloat16 --> BF16, shape = {1024, 151860}
INFO:hf-to-gguf:blk.0.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.0.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.0.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.0.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.0.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.0.attn_k.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.0.attn_k.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.0.attn_output.weight,         torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.0.attn_q.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.0.attn_q.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.0.attn_v.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.0.attn_v.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.1.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.1.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.1.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.1.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.1.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.1.attn_k.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.1.attn_k.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.1.attn_output.weight,         torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.1.attn_q.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.1.attn_q.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.1.attn_v.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.1.attn_v.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.10.attn_norm.weight,          torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.10.ffn_down.weight,           torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.10.ffn_gate.weight,           torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.10.ffn_up.weight,             torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.10.ffn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.10.attn_k.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.10.attn_k.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.10.attn_output.weight,        torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.10.attn_q.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.10.attn_q.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.10.attn_v.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.10.attn_v.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.11.attn_norm.weight,          torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.11.ffn_down.weight,           torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.11.ffn_gate.weight,           torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.11.ffn_up.weight,             torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.11.ffn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.11.attn_k.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.11.attn_k.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.11.attn_output.weight,        torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.11.attn_q.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.11.attn_q.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.11.attn_v.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.11.attn_v.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.12.attn_norm.weight,          torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.12.ffn_down.weight,           torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.12.ffn_gate.weight,           torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.12.ffn_up.weight,             torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.12.ffn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.12.attn_k.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.12.attn_k.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.12.attn_output.weight,        torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.12.attn_q.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.12.attn_q.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.12.attn_v.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.12.attn_v.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.13.attn_norm.weight,          torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.13.ffn_down.weight,           torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.13.ffn_gate.weight,           torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.13.ffn_up.weight,             torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.13.ffn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.13.attn_k.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.13.attn_k.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.13.attn_output.weight,        torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.13.attn_q.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.13.attn_q.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.13.attn_v.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.13.attn_v.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.14.attn_norm.weight,          torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.14.ffn_down.weight,           torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.14.ffn_gate.weight,           torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.14.ffn_up.weight,             torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.14.ffn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.14.attn_k.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.14.attn_k.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.14.attn_output.weight,        torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.14.attn_q.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.14.attn_q.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.14.attn_v.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.14.attn_v.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.15.attn_norm.weight,          torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.15.ffn_down.weight,           torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.15.ffn_gate.weight,           torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.15.ffn_up.weight,             torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.15.ffn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.15.attn_k.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.15.attn_k.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.15.attn_output.weight,        torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.15.attn_q.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.15.attn_q.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.15.attn_v.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.15.attn_v.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.16.attn_norm.weight,          torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.16.ffn_down.weight,           torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.16.ffn_gate.weight,           torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.16.ffn_up.weight,             torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.16.ffn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.16.attn_k.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.16.attn_k.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.16.attn_output.weight,        torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.16.attn_q.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.16.attn_q.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.16.attn_v.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.16.attn_v.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.17.attn_norm.weight,          torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.17.ffn_down.weight,           torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.17.ffn_gate.weight,           torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.17.ffn_up.weight,             torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.17.ffn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.17.attn_k.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.17.attn_k.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.17.attn_output.weight,        torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.17.attn_q.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.17.attn_q.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.17.attn_v.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.17.attn_v.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.18.attn_norm.weight,          torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.18.ffn_down.weight,           torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.18.ffn_gate.weight,           torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.18.ffn_up.weight,             torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.18.ffn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.18.attn_k.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.18.attn_k.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.18.attn_output.weight,        torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.18.attn_q.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.18.attn_q.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.18.attn_v.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.18.attn_v.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.19.attn_norm.weight,          torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.19.ffn_down.weight,           torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.19.ffn_gate.weight,           torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.19.ffn_up.weight,             torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.19.ffn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.19.attn_k.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.19.attn_k.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.19.attn_output.weight,        torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.19.attn_q.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.19.attn_q.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.19.attn_v.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.19.attn_v.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.2.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.2.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.2.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.2.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.2.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.2.attn_k.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.2.attn_k.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.2.attn_output.weight,         torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.2.attn_q.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.2.attn_q.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.2.attn_v.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.2.attn_v.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.20.attn_norm.weight,          torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.20.ffn_down.weight,           torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.20.ffn_gate.weight,           torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.20.ffn_up.weight,             torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.20.ffn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.20.attn_k.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.20.attn_k.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.20.attn_output.weight,        torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.20.attn_q.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.20.attn_q.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.20.attn_v.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.20.attn_v.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.21.attn_norm.weight,          torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.21.ffn_down.weight,           torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.21.ffn_gate.weight,           torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.21.ffn_up.weight,             torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.21.ffn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.21.attn_k.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.21.attn_k.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.21.attn_output.weight,        torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.21.attn_q.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.21.attn_q.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.21.attn_v.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.21.attn_v.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.22.attn_norm.weight,          torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.22.ffn_down.weight,           torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.22.ffn_gate.weight,           torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.22.ffn_up.weight,             torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.22.ffn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.22.attn_k.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.22.attn_k.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.22.attn_output.weight,        torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.22.attn_q.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.22.attn_q.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.22.attn_v.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.22.attn_v.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.23.attn_norm.weight,          torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.23.ffn_down.weight,           torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.23.ffn_gate.weight,           torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.23.ffn_up.weight,             torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.23.ffn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.23.attn_k.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.23.attn_k.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.23.attn_output.weight,        torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.23.attn_q.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.23.attn_q.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.23.attn_v.bias,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.23.attn_v.weight,             torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.3.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.3.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.3.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.3.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.3.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.3.attn_k.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.3.attn_k.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.3.attn_output.weight,         torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.3.attn_q.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.3.attn_q.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.3.attn_v.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.3.attn_v.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.4.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.4.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.4.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.4.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.4.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.4.attn_k.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.4.attn_k.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.4.attn_output.weight,         torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.4.attn_q.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.4.attn_q.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.4.attn_v.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.4.attn_v.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.5.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.5.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.5.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.5.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.5.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.5.attn_k.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.5.attn_k.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.5.attn_output.weight,         torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.5.attn_q.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.5.attn_q.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.5.attn_v.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.5.attn_v.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.6.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.6.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.6.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.6.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.6.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.6.attn_k.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.6.attn_k.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.6.attn_output.weight,         torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.6.attn_q.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.6.attn_q.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.6.attn_v.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.6.attn_v.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.7.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.7.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.7.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.7.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.7.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.7.attn_k.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.7.attn_k.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.7.attn_output.weight,         torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.7.attn_q.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.7.attn_q.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.7.attn_v.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.7.attn_v.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.8.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.8.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.8.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.8.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.8.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.8.attn_k.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.8.attn_k.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.8.attn_output.weight,         torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.8.attn_q.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.8.attn_q.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.8.attn_v.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.8.attn_v.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.9.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.9.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {2816, 1024}
INFO:hf-to-gguf:blk.9.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.9.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {1024, 2816}
INFO:hf-to-gguf:blk.9.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.9.attn_k.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.9.attn_k.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.9.attn_output.weight,         torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.9.attn_q.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.9.attn_q.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.9.attn_v.bias,                torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:blk.9.attn_v.weight,              torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:mm_proj.bias,                     torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:mm_proj.weight,                   torch.bfloat16 --> BF16, shape = {1024, 1024}
INFO:hf-to-gguf:output_norm.weight,               torch.bfloat16 --> F32, shape = {1024}
INFO:hf-to-gguf:vis.blk.0.attn.proj.bias,         torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.0.attn.proj.weight,       torch.bfloat16 --> BF16, shape = {768, 768}
INFO:hf-to-gguf:vis.blk.0.attn.qkv.bias,          torch.bfloat16 --> F32, shape = {2304}
INFO:hf-to-gguf:vis.blk.0.attn.qkv.weight,        torch.bfloat16 --> BF16, shape = {768, 2304}
INFO:hf-to-gguf:vis.blk.0.attn.rel_pos_h,         torch.bfloat16 --> F32, shape = {64, 27}
INFO:hf-to-gguf:vis.blk.0.attn.rel_pos_w,         torch.bfloat16 --> F32, shape = {64, 27}
INFO:hf-to-gguf:vis.blk.0.mlp.lin1.bias,          torch.bfloat16 --> F32, shape = {3072}
INFO:hf-to-gguf:vis.blk.0.mlp.lin1.weight,        torch.bfloat16 --> BF16, shape = {768, 3072}
INFO:hf-to-gguf:vis.blk.0.mlp.lin2.bias,          torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.0.mlp.lin2.weight,        torch.bfloat16 --> BF16, shape = {3072, 768}
INFO:hf-to-gguf:vis.blk.0.norm1.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.0.norm1.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.0.norm2.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.0.norm2.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.1.attn.proj.bias,         torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.1.attn.proj.weight,       torch.bfloat16 --> BF16, shape = {768, 768}
INFO:hf-to-gguf:vis.blk.1.attn.qkv.bias,          torch.bfloat16 --> F32, shape = {2304}
INFO:hf-to-gguf:vis.blk.1.attn.qkv.weight,        torch.bfloat16 --> BF16, shape = {768, 2304}
INFO:hf-to-gguf:vis.blk.1.attn.rel_pos_h,         torch.bfloat16 --> F32, shape = {64, 27}
INFO:hf-to-gguf:vis.blk.1.attn.rel_pos_w,         torch.bfloat16 --> F32, shape = {64, 27}
INFO:hf-to-gguf:vis.blk.1.mlp.lin1.bias,          torch.bfloat16 --> F32, shape = {3072}
INFO:hf-to-gguf:vis.blk.1.mlp.lin1.weight,        torch.bfloat16 --> BF16, shape = {768, 3072}
INFO:hf-to-gguf:vis.blk.1.mlp.lin2.bias,          torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.1.mlp.lin2.weight,        torch.bfloat16 --> BF16, shape = {3072, 768}
INFO:hf-to-gguf:vis.blk.1.norm1.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.1.norm1.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.1.norm2.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.1.norm2.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.10.attn.proj.bias,        torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.10.attn.proj.weight,      torch.bfloat16 --> BF16, shape = {768, 768}
INFO:hf-to-gguf:vis.blk.10.attn.qkv.bias,         torch.bfloat16 --> F32, shape = {2304}
INFO:hf-to-gguf:vis.blk.10.attn.qkv.weight,       torch.bfloat16 --> BF16, shape = {768, 2304}
INFO:hf-to-gguf:vis.blk.10.attn.rel_pos_h,        torch.bfloat16 --> F32, shape = {64, 27}
INFO:hf-to-gguf:vis.blk.10.attn.rel_pos_w,        torch.bfloat16 --> F32, shape = {64, 27}
INFO:hf-to-gguf:vis.blk.10.mlp.lin1.bias,         torch.bfloat16 --> F32, shape = {3072}
INFO:hf-to-gguf:vis.blk.10.mlp.lin1.weight,       torch.bfloat16 --> BF16, shape = {768, 3072}
INFO:hf-to-gguf:vis.blk.10.mlp.lin2.bias,         torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.10.mlp.lin2.weight,       torch.bfloat16 --> BF16, shape = {3072, 768}
INFO:hf-to-gguf:vis.blk.10.norm1.bias,            torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.10.norm1.weight,          torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.10.norm2.bias,            torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.10.norm2.weight,          torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.11.attn.proj.bias,        torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.11.attn.proj.weight,      torch.bfloat16 --> BF16, shape = {768, 768}
INFO:hf-to-gguf:vis.blk.11.attn.qkv.bias,         torch.bfloat16 --> F32, shape = {2304}
INFO:hf-to-gguf:vis.blk.11.attn.qkv.weight,       torch.bfloat16 --> BF16, shape = {768, 2304}
INFO:hf-to-gguf:vis.blk.11.attn.rel_pos_h,        torch.bfloat16 --> F32, shape = {64, 127}
INFO:hf-to-gguf:vis.blk.11.attn.rel_pos_w,        torch.bfloat16 --> F32, shape = {64, 127}
INFO:hf-to-gguf:vis.blk.11.mlp.lin1.bias,         torch.bfloat16 --> F32, shape = {3072}
INFO:hf-to-gguf:vis.blk.11.mlp.lin1.weight,       torch.bfloat16 --> BF16, shape = {768, 3072}
INFO:hf-to-gguf:vis.blk.11.mlp.lin2.bias,         torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.11.mlp.lin2.weight,       torch.bfloat16 --> BF16, shape = {3072, 768}
INFO:hf-to-gguf:vis.blk.11.norm1.bias,            torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.11.norm1.weight,          torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.11.norm2.bias,            torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.11.norm2.weight,          torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.2.attn.proj.bias,         torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.2.attn.proj.weight,       torch.bfloat16 --> BF16, shape = {768, 768}
INFO:hf-to-gguf:vis.blk.2.attn.qkv.bias,          torch.bfloat16 --> F32, shape = {2304}
INFO:hf-to-gguf:vis.blk.2.attn.qkv.weight,        torch.bfloat16 --> BF16, shape = {768, 2304}
INFO:hf-to-gguf:vis.blk.2.attn.rel_pos_h,         torch.bfloat16 --> F32, shape = {64, 127}
INFO:hf-to-gguf:vis.blk.2.attn.rel_pos_w,         torch.bfloat16 --> F32, shape = {64, 127}
INFO:hf-to-gguf:vis.blk.2.mlp.lin1.bias,          torch.bfloat16 --> F32, shape = {3072}
INFO:hf-to-gguf:vis.blk.2.mlp.lin1.weight,        torch.bfloat16 --> BF16, shape = {768, 3072}
INFO:hf-to-gguf:vis.blk.2.mlp.lin2.bias,          torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.2.mlp.lin2.weight,        torch.bfloat16 --> BF16, shape = {3072, 768}
INFO:hf-to-gguf:vis.blk.2.norm1.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.2.norm1.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.2.norm2.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.2.norm2.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.3.attn.proj.bias,         torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.3.attn.proj.weight,       torch.bfloat16 --> BF16, shape = {768, 768}
INFO:hf-to-gguf:vis.blk.3.attn.qkv.bias,          torch.bfloat16 --> F32, shape = {2304}
INFO:hf-to-gguf:vis.blk.3.attn.qkv.weight,        torch.bfloat16 --> BF16, shape = {768, 2304}
INFO:hf-to-gguf:vis.blk.3.attn.rel_pos_h,         torch.bfloat16 --> F32, shape = {64, 27}
INFO:hf-to-gguf:vis.blk.3.attn.rel_pos_w,         torch.bfloat16 --> F32, shape = {64, 27}
INFO:hf-to-gguf:vis.blk.3.mlp.lin1.bias,          torch.bfloat16 --> F32, shape = {3072}
INFO:hf-to-gguf:vis.blk.3.mlp.lin1.weight,        torch.bfloat16 --> BF16, shape = {768, 3072}
INFO:hf-to-gguf:vis.blk.3.mlp.lin2.bias,          torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.3.mlp.lin2.weight,        torch.bfloat16 --> BF16, shape = {3072, 768}
INFO:hf-to-gguf:vis.blk.3.norm1.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.3.norm1.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.3.norm2.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.3.norm2.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.4.attn.proj.bias,         torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.4.attn.proj.weight,       torch.bfloat16 --> BF16, shape = {768, 768}
INFO:hf-to-gguf:vis.blk.4.attn.qkv.bias,          torch.bfloat16 --> F32, shape = {2304}
INFO:hf-to-gguf:vis.blk.4.attn.qkv.weight,        torch.bfloat16 --> BF16, shape = {768, 2304}
INFO:hf-to-gguf:vis.blk.4.attn.rel_pos_h,         torch.bfloat16 --> F32, shape = {64, 27}
INFO:hf-to-gguf:vis.blk.4.attn.rel_pos_w,         torch.bfloat16 --> F32, shape = {64, 27}
INFO:hf-to-gguf:vis.blk.4.mlp.lin1.bias,          torch.bfloat16 --> F32, shape = {3072}
INFO:hf-to-gguf:vis.blk.4.mlp.lin1.weight,        torch.bfloat16 --> BF16, shape = {768, 3072}
INFO:hf-to-gguf:vis.blk.4.mlp.lin2.bias,          torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.4.mlp.lin2.weight,        torch.bfloat16 --> BF16, shape = {3072, 768}
INFO:hf-to-gguf:vis.blk.4.norm1.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.4.norm1.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.4.norm2.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.4.norm2.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.5.attn.proj.bias,         torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.5.attn.proj.weight,       torch.bfloat16 --> BF16, shape = {768, 768}
INFO:hf-to-gguf:vis.blk.5.attn.qkv.bias,          torch.bfloat16 --> F32, shape = {2304}
INFO:hf-to-gguf:vis.blk.5.attn.qkv.weight,        torch.bfloat16 --> BF16, shape = {768, 2304}
INFO:hf-to-gguf:vis.blk.5.attn.rel_pos_h,         torch.bfloat16 --> F32, shape = {64, 127}
INFO:hf-to-gguf:vis.blk.5.attn.rel_pos_w,         torch.bfloat16 --> F32, shape = {64, 127}
INFO:hf-to-gguf:vis.blk.5.mlp.lin1.bias,          torch.bfloat16 --> F32, shape = {3072}
INFO:hf-to-gguf:vis.blk.5.mlp.lin1.weight,        torch.bfloat16 --> BF16, shape = {768, 3072}
INFO:hf-to-gguf:vis.blk.5.mlp.lin2.bias,          torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.5.mlp.lin2.weight,        torch.bfloat16 --> BF16, shape = {3072, 768}
INFO:hf-to-gguf:vis.blk.5.norm1.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.5.norm1.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.5.norm2.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.5.norm2.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.6.attn.proj.bias,         torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.6.attn.proj.weight,       torch.bfloat16 --> BF16, shape = {768, 768}
INFO:hf-to-gguf:vis.blk.6.attn.qkv.bias,          torch.bfloat16 --> F32, shape = {2304}
INFO:hf-to-gguf:vis.blk.6.attn.qkv.weight,        torch.bfloat16 --> BF16, shape = {768, 2304}
INFO:hf-to-gguf:vis.blk.6.attn.rel_pos_h,         torch.bfloat16 --> F32, shape = {64, 27}
INFO:hf-to-gguf:vis.blk.6.attn.rel_pos_w,         torch.bfloat16 --> F32, shape = {64, 27}
INFO:hf-to-gguf:vis.blk.6.mlp.lin1.bias,          torch.bfloat16 --> F32, shape = {3072}
INFO:hf-to-gguf:vis.blk.6.mlp.lin1.weight,        torch.bfloat16 --> BF16, shape = {768, 3072}
INFO:hf-to-gguf:vis.blk.6.mlp.lin2.bias,          torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.6.mlp.lin2.weight,        torch.bfloat16 --> BF16, shape = {3072, 768}
INFO:hf-to-gguf:vis.blk.6.norm1.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.6.norm1.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.6.norm2.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.6.norm2.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.7.attn.proj.bias,         torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.7.attn.proj.weight,       torch.bfloat16 --> BF16, shape = {768, 768}
INFO:hf-to-gguf:vis.blk.7.attn.qkv.bias,          torch.bfloat16 --> F32, shape = {2304}
INFO:hf-to-gguf:vis.blk.7.attn.qkv.weight,        torch.bfloat16 --> BF16, shape = {768, 2304}
INFO:hf-to-gguf:vis.blk.7.attn.rel_pos_h,         torch.bfloat16 --> F32, shape = {64, 27}
INFO:hf-to-gguf:vis.blk.7.attn.rel_pos_w,         torch.bfloat16 --> F32, shape = {64, 27}
INFO:hf-to-gguf:vis.blk.7.mlp.lin1.bias,          torch.bfloat16 --> F32, shape = {3072}
INFO:hf-to-gguf:vis.blk.7.mlp.lin1.weight,        torch.bfloat16 --> BF16, shape = {768, 3072}
INFO:hf-to-gguf:vis.blk.7.mlp.lin2.bias,          torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.7.mlp.lin2.weight,        torch.bfloat16 --> BF16, shape = {3072, 768}
INFO:hf-to-gguf:vis.blk.7.norm1.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.7.norm1.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.7.norm2.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.7.norm2.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.8.attn.proj.bias,         torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.8.attn.proj.weight,       torch.bfloat16 --> BF16, shape = {768, 768}
INFO:hf-to-gguf:vis.blk.8.attn.qkv.bias,          torch.bfloat16 --> F32, shape = {2304}
INFO:hf-to-gguf:vis.blk.8.attn.qkv.weight,        torch.bfloat16 --> BF16, shape = {768, 2304}
INFO:hf-to-gguf:vis.blk.8.attn.rel_pos_h,         torch.bfloat16 --> F32, shape = {64, 127}
INFO:hf-to-gguf:vis.blk.8.attn.rel_pos_w,         torch.bfloat16 --> F32, shape = {64, 127}
INFO:hf-to-gguf:vis.blk.8.mlp.lin1.bias,          torch.bfloat16 --> F32, shape = {3072}
INFO:hf-to-gguf:vis.blk.8.mlp.lin1.weight,        torch.bfloat16 --> BF16, shape = {768, 3072}
INFO:hf-to-gguf:vis.blk.8.mlp.lin2.bias,          torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.8.mlp.lin2.weight,        torch.bfloat16 --> BF16, shape = {3072, 768}
INFO:hf-to-gguf:vis.blk.8.norm1.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.8.norm1.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.8.norm2.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.8.norm2.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.9.attn.proj.bias,         torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.9.attn.proj.weight,       torch.bfloat16 --> BF16, shape = {768, 768}
INFO:hf-to-gguf:vis.blk.9.attn.qkv.bias,          torch.bfloat16 --> F32, shape = {2304}
INFO:hf-to-gguf:vis.blk.9.attn.qkv.weight,        torch.bfloat16 --> BF16, shape = {768, 2304}
INFO:hf-to-gguf:vis.blk.9.attn.rel_pos_h,         torch.bfloat16 --> F32, shape = {64, 27}
INFO:hf-to-gguf:vis.blk.9.attn.rel_pos_w,         torch.bfloat16 --> F32, shape = {64, 27}
INFO:hf-to-gguf:vis.blk.9.mlp.lin1.bias,          torch.bfloat16 --> F32, shape = {3072}
INFO:hf-to-gguf:vis.blk.9.mlp.lin1.weight,        torch.bfloat16 --> BF16, shape = {768, 3072}
INFO:hf-to-gguf:vis.blk.9.mlp.lin2.bias,          torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.9.mlp.lin2.weight,        torch.bfloat16 --> BF16, shape = {3072, 768}
INFO:hf-to-gguf:vis.blk.9.norm1.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.9.norm1.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.9.norm2.bias,             torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.blk.9.norm2.weight,           torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis.neck.0.weight,                torch.bfloat16 --> BF16, shape = {768, 256}
INFO:hf-to-gguf:vis.neck.1.bias,                  torch.bfloat16 --> F32, shape = {256}
INFO:hf-to-gguf:vis.neck.1.weight,                torch.bfloat16 --> F32, shape = {256}
INFO:hf-to-gguf:vis.neck.2.weight,                torch.bfloat16 --> BF16, shape = {3, 3, 256, 256}
INFO:hf-to-gguf:vis.neck.3.bias,                  torch.bfloat16 --> F32, shape = {256}
INFO:hf-to-gguf:vis.neck.3.weight,                torch.bfloat16 --> F32, shape = {256}
INFO:hf-to-gguf:vis.net_2.weight,                 torch.bfloat16 --> BF16, shape = {3, 3, 256, 512}
INFO:hf-to-gguf:vis.net_3.weight,                 torch.bfloat16 --> BF16, shape = {3, 3, 512, 1024}
INFO:hf-to-gguf:vis_patch_embd.proj.bias,         torch.bfloat16 --> F32, shape = {768}
INFO:hf-to-gguf:vis_patch_embd.proj.weight,       torch.bfloat16 --> BF16, shape = {16, 16, 3, 768}
INFO:hf-to-gguf:vis_pos_embd,                     torch.bfloat16 --> F32, shape = {768, 64, 64}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 32768
INFO:hf-to-gguf:gguf: embedding length = 1024
INFO:hf-to-gguf:gguf: feed forward length = 2816
INFO:hf-to-gguf:gguf: head count = 16
INFO:hf-to-gguf:gguf: key-value head count = 16
INFO:hf-to-gguf:gguf: rope theta = 1000000.0
INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06
INFO:hf-to-gguf:gguf: file type = 32
INFO:hf-to-gguf:gguf: (got) vis_n_layer = 12
INFO:hf-to-gguf:gguf: (got) vis_n_embd = 768
INFO:hf-to-gguf:gguf: (got) vis_n_head = 12
INFO:hf-to-gguf:gguf: (got) vis_intermediate_size = 3072
INFO:hf-to-gguf:gguf: (got) vis_img_token_len = 256
INFO:hf-to-gguf:gguf: (got) vis_in_channels = 3
INFO:hf-to-gguf:gguf: (got) vis_out_channels = 256
INFO:hf-to-gguf:gguf: (got) vis_img_size = 1024
INFO:hf-to-gguf:gguf: (got) vis_patch_size = 16
INFO:hf-to-gguf:gguf: (got) vis_window_size = 14
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Adding 151387 merge(s).
INFO:gguf.vocab:Setting special token type bos to 151643
INFO:gguf.vocab:Setting special token type eos to 151643
INFO:gguf.vocab:Setting special token type im_start to 151857
INFO:gguf.vocab:Setting special token type im_end to 151858
INFO:gguf.vocab:Setting special token type im_patch to 151859
INFO:gguf.vocab:Setting special token type unk to 151643
INFO:hf-to-gguf:Set model quantization version
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:/Users/sulei03/Documents/llama.cpp/data/output/GOT-OCR2_0-716M-BF16.gguf: n_tensors = 472, total_size = 1.4G
Writing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.44G/1.44G [00:20<00:00, 70.6Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to /Users/sulei03/Documents/llama.cpp/data/output/GOT-OCR2_0-716M-BF16.gguf